# -*- coding: utf-8 -*-
"""Crop Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hOiXrjDWHFHAlTkSb3xaug-mjTgMD12v

# Crop Recommendation System to Maximize Crop Yield Using Machine Learning Techniques                   
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Importing of libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

df_crop = pd.read_csv('/content/drive/MyDrive/Crop_recommendation.csv')
df_crop

df_crop.describe()

df_crop.shape
df_crop.info()

"""## Exploratory Data Analysis"""

plt.figure(1, figsize=(15,14))
n = 0
for x in ['N','P','K','temperature','humidity','ph','rainfall']:
    n += 1
    plt.subplot(3,3,n)
    plt.subplots_adjust(hspace=0.2, wspace=0.3)
    sns.distplot(df_crop[x], bins=20)
plt.show()

def get_region(row):
    if row['ph'] >= 5.0 and row['ph'] <= 6.5 and row['temperature'] >= 20 and row['temperature'] <= 22.5 and row['rainfall'] >= 150 and row['rainfall'] <= 250:
        return 'Northern India'
    elif row['ph'] >= 5.2 and row['ph'] <= 7.5 and row['temperature'] >= 22.5 and row['temperature'] <= 25 and row['rainfall'] >= 150 and row['rainfall'] <= 250:
        return 'North Eastern India'
    elif row['ph'] >= 5.2 and row['ph'] <= 7.5 and row['temperature'] >= 25 and row['temperature'] <= 27.5 and row['rainfall'] >= 100 and row['rainfall'] <= 150:
        return 'Eastern India'
    elif row['ph'] >= 6.5 and row['ph'] <= 8.4 and row['temperature'] >= 25 and row['temperature'] <= 27.5 and row['rainfall'] >= 100 and row['rainfall'] <= 150:
        return 'Central India'
    elif row['ph'] >= 6.5 and row['ph'] <= 8.4 and row['temperature'] >= 25 and row['temperature'] <= 27.5 and row['rainfall'] >= 150 and row['rainfall'] <= 250:
        return 'Western India'
    elif row['ph'] >= 5.2 and row['ph'] <= 7.5 and row['temperature'] > 27.5 and row['temperature'] >= 50 and row['rainfall'] <= 100:
        return 'Southern India'
    else:
        return 'Other'

    return location
df_crop['region'] = df_crop.apply(get_region, axis=1)

df_crop = pd.get_dummies(df_crop, columns=['region'], prefix=['region'])
df_crop

"""## Data Splitting"""

X = df_crop.drop(['label'], axis=1)
y = df_crop['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Variables for Accuracy and Models
acc = []
model = []

"""## Random Forests"""

from sklearn.ensemble import RandomForestClassifier

RandomForest = RandomForestClassifier(criterion='gini', n_estimators=100, random_state=42, max_depth=None)
RandomForest.fit(X_train, y_train)

y_pred = RandomForest.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy * 100)
acc.append(accuracy)
model.append('Random Forest')

report = classification_report(y_test, y_pred)
print('Classification report:\n', report)

# Confusion Metrics
from sklearn.metrics import confusion_matrix

# Assuming y_test and y_pred are your true and predicted classes, respectively
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using a heatmap
plt.figure(figsize=(8,7))
sns.heatmap(cm, annot=True, cmap='Blues')
plt.xlabel('Predicted classes')
plt.ylabel('True classes')
plt.show()

# Experiment 1
RandomForest = RandomForestClassifier(criterion='entropy', n_estimators=50, random_state=42, max_depth=5)
RandomForest.fit(X_train, y_train)

y_pred = RandomForest.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy * 100)

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

DecisonTree = DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=None)
DecisonTree.fit(X_train, y_train)

y_pred = DecisonTree.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy * 100)
acc.append(accuracy)
model.append('Decision Tree')

report = classification_report(y_test, y_pred)
print('Classification report:\n', report)

# Assuming y_test and y_pred are your true and predicted classes, respectively
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using a heatmap
plt.figure(figsize=(8,7))
sns.heatmap(cm, annot=True, cmap='Blues')
plt.xlabel('Predicted classes')
plt.ylabel('True classes')
plt.show()

# Experiment 1
DecisonTree = DecisionTreeClassifier(criterion='entropy',splitter='random',max_depth=5)
DecisonTree.fit(X_train, y_train)

y_pred = DecisonTree.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy * 100)

"""## K-Nearest Neighbors"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=22, weights='distance', algorithm='auto')
knn.fit(X_train,y_train)

y_pred = knn.predict(X_test)
knn_testing_acc = metrics.accuracy_score(y_test, y_pred)
# acc.append(testing_acc)
# model.append('K Nearest Neighbours')
print("KNN Accuracy: ", knn_testing_acc*100)

acc.append(knn_testing_acc)
model.append('KNN')

report = classification_report(y_test, y_pred)
print('Classification report:\n', report)

# Assuming y_test and y_pred are your true and predicted classes, respectively
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using a heatmap
plt.figure(figsize=(8,7))
sns.heatmap(cm, annot=True, cmap='Blues')
plt.xlabel('Predicted classes')
plt.ylabel('True classes')
plt.show()

knn = KNeighborsClassifier(n_neighbors=22, weights='uniform', algorithm='kd_tree')
knn.fit(X_train,y_train)

y_pred = knn.predict(X_test)
knn_testing_acc = metrics.accuracy_score(y_test, y_pred)
print("KNN Accuracy: ", knn_testing_acc*100)

"""## Test Algorithm"""

plt.figure(figsize=[10,5],dpi = 100)
plt.title('Accuracy Comparison')
plt.xlabel('Accuracy')
plt.ylabel('Algorithm')
sns.barplot(x = acc,y = model, palette='Blues')

import numpy as np
test_rc = np.array([[107,34,32,26.774637,66.413269,6.780064,177.774507,0,0,0,0,0,1]])
print(test_rc)

"""### Test Algorithm"""

y_pred_1 = RandomForest.predict(test_rc)
print(y_pred_1)

"""## Deploy Model"""

import pickle

with open('rf_model.pkl', 'wb') as file:
    pickle.dump(RandomForest, file)

"""## Crops per Region Summary"""

sample = df_crop.groupby('label')[['region_Central India', 'region_Eastern India', 'region_North Eastern India', 'region_Northern India', 'region_Western India','region_Other']].sum()
sample

import numpy as np

import numpy as np  # Ensure numpy is imported

CI = sample.groupby('label')[['region_Central India']].sum()
CI = CI.replace(0, np.nan)  # Use numpy's nan
CI = CI.dropna()
CI

EI = sample.groupby('label')[['region_Eastern India']].sum()
EI = EI.replace(0, np.nan)  # Replace 0 with numpy's nan
EI = EI.dropna()  # Drop rows where all elements are NaN
EI

NEI = sample.groupby('label')[['region_North Eastern India']].sum()
NEI = NEI.replace(0, np.nan)  # Replace 0 with numpy's NaN
NEI = NEI.dropna()  # Drop rows with NaN values
NEI

NI = sample.groupby('label')[['region_Northern India']].sum()
NI = NI.replace(0, np.nan)  # Use numpy's nan instead of pd.np.nan
NI = NI.dropna()  # Drop rows with NaN values
NI

WI = sample.groupby('label')[['region_Western India']].sum()
WI = WI.replace(0, np.nan)
WI=WI.dropna()
WI

combined_df = pd.concat([CI, EI, NEI, NI, WI])
combined_df

